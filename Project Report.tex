\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    \usepackage{algorithm}
    \usepackage{algorithmicx}
    \usepackage{algpseudocode}
    
    \renewcommand{\algorithmicensure}{\textbf{Initialize:}}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Project Report}
    \author{Ze'en Chi}
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    \hypertarget{compute-the-theoretically-maximized-expectation-given-theta_js}{%
\section{\texorpdfstring{Compute the theoretically maximized expectation
given
\(\theta_j\)'s}{Compute the theoretically maximized expectation given \textbackslash theta\_j's}}\label{compute-the-theoretically-maximized-expectation-given-theta_js}}

Since \(r_{I(t)}\sim{\rm Bern}(\theta_{I(t)})\) and all the
\(\theta_j\)'s are already given, we can easily compute the maximized
expectation of \(\sum\limits_{t=1}^Nr_{I(t)}\).

\[
\mathbb{E}\left(\sum\limits_{t=1}^Nr_{I(t)}\right)=\sum\limits_{t=1}^N\mathbb{E}(r_{I(t)})=\sum\limits_{t=1}^N\theta_{I(t)}
\]

Now that the given values of \(\theta_j\)'s are
\(\theta_1=0.8,\theta_2=0.6,\theta_3=0.5\), in order to obtain the
maximized expectation, it suffices to pull arm 1 each time. So the
aggeragate rewards over \(N\) times is \(N*\theta_1=0.8N\).

Therefore the oracle value is \(0.8N=4800\).

    \hypertarget{classical-bandit-algorithms}{%
\section{Classical bandit
algorithms}\label{classical-bandit-algorithms}}

In this section, we will find out the performance of the given
algorithms: \(\epsilon\)-greedy, UCB and Thompson sampling.

As for each algorithm, in terms of different parameters, there are three
steps in total:

\begin{itemize}
\item
  Algorithm implementation.
\item
  Compute the outcomes over 200 independent experiments.
\item
  Regret analysis, including mean and variance of the gap between the
  algorithm outputs and the oracle value.
\end{itemize}

We use the given parameters \(\theta_1=0.8,\theta_2=0.6,\theta_3=0.5\)
to generate \(r_{I(t)}\) at each time slot \(t\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{51}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{from} \PY{n+nn}{numpy} \PY{k+kn}{import} \PY{n}{random}
\PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k+kn}{import} \PY{n}{pyplot} \PY{k}{as} \PY{n}{plt}
\PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{core}\PY{n+nn}{.}\PY{n+nn}{pylabtools} \PY{k+kn}{import} \PY{n}{figsize}
\PY{k+kn}{import} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{stats} \PY{k}{as} \PY{n+nn}{stats}
\PY{k+kn}{import} \PY{n+nn}{os}
\PY{k+kn}{from} \PY{n+nn}{PIL} \PY{k+kn}{import} \PY{n}{Image}
\PY{n}{N} \PY{o}{=} \PY{l+m+mi}{6000}
\PY{n}{EPS} \PY{o}{=} \PY{l+m+mi}{200}
\PY{n}{prob} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.8}\PY{p}{,} \PY{l+m+mf}{0.6}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{56}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{Bandit1}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{prob} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{prob}\PY{p}{)}  \PY{c+c1}{\PYZsh{} The true rate of reward of each arm}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{count} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{est\PYZus{}values} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{reward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{arm}\PY{p}{)}\PY{p}{:}
        \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Generate the reward by Bernoulli trial\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{k}{return} \PY{n}{random}\PY{o}{.}\PY{n}{binomial}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{prob}\PY{p}{[}\PY{n}{arm}\PY{p}{]}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{e\PYZus{}greedy}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{epsilon}\PY{p}{)}\PY{p}{:}
        \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Determine whether to explore or exploit\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{n}{rand} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{random}\PY{p}{(}\PY{p}{)}
        \PY{k}{if} \PY{n}{rand} \PY{o}{\PYZlt{}} \PY{n}{epsilon}\PY{p}{:}
            \PY{k}{return} \PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Exploration}
        \PY{k}{else}\PY{p}{:}
            \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{est\PYZus{}values}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Exploitation}

    \PY{k}{def} \PY{n+nf}{UCB}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{c}\PY{p}{,}\PY{n}{t}\PY{p}{)}\PY{p}{:}
        \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Compute the comprehensive value of exploration and exploitation to determine which arm to choose\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{n}{EE\PYZus{}mix} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{est\PYZus{}values}\PY{o}{+}\PY{n}{c}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{t}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{/}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{count}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{EE\PYZus{}mix}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{update}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{arm}\PY{p}{,} \PY{n}{R}\PY{p}{)}\PY{p}{:}
        \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Update the number of being selected and the estimated reward of the arm\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{count}\PY{p}{[}\PY{n}{arm}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{est\PYZus{}values}\PY{p}{[}\PY{n}{arm}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{R}\PY{o}{\PYZhy{}}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{est\PYZus{}values}\PY{p}{[}\PY{n}{arm}\PY{p}{]}\PY{p}{)}\PY{o}{/}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{count}\PY{p}{[}\PY{n}{arm}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{epsilon-greedy-algorithm}{%
\subsection{\texorpdfstring{\(\epsilon\)-greedy
Algorithm}{\textbackslash epsilon-greedy Algorithm}}\label{epsilon-greedy-algorithm}}

    \hypertarget{implementation}{%
\subsubsection{Implementation}\label{implementation}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{epsilon} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{l+m+mf}{0.4}\PY{p}{,} \PY{l+m+mf}{0.6}\PY{p}{,} \PY{l+m+mf}{0.8}\PY{p}{]}
\PY{k}{def} \PY{n+nf}{experiment1}\PY{p}{(}\PY{n}{bandit}\PY{p}{,} \PY{n}{epsilon}\PY{p}{)}\PY{p}{:}
    \PY{n+nb}{sum} \PY{o}{=} \PY{l+m+mi}{0} \PY{c+c1}{\PYZsh{} The aggreagte reward of a certain experiment}
    \PY{n}{tmp\PYZus{}cum\PYZus{}reward} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{c+c1}{\PYZsh{} The average cumulative reward at each time slot}
    \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{N}\PY{p}{)}\PY{p}{:}
        \PY{n}{arm} \PY{o}{=} \PY{n}{bandit}\PY{o}{.}\PY{n}{e\PYZus{}greedy}\PY{p}{(}\PY{n}{epsilon}\PY{p}{)}
        \PY{n}{reward} \PY{o}{=} \PY{n}{bandit}\PY{o}{.}\PY{n}{reward}\PY{p}{(}\PY{n}{arm}\PY{p}{)}
        \PY{n}{bandit}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{n}{arm}\PY{p}{,} \PY{n}{reward}\PY{p}{)}
        \PY{n+nb}{sum} \PY{o}{+}\PY{o}{=} \PY{n}{reward}
        \PY{n}{tmp\PYZus{}cum\PYZus{}reward}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{n}{t}\PY{o}{*}\PY{n}{tmp\PYZus{}cum\PYZus{}reward}\PY{p}{[}\PY{n}{t}\PY{p}{]}\PY{o}{+}\PY{n}{reward}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{t}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{p}{[}\PY{n+nb}{sum}\PY{p}{,} \PY{l+m+mi}{4800}\PY{o}{\PYZhy{}}\PY{n+nb}{sum}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{tmp\PYZus{}cum\PYZus{}reward}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{simulative-outcomes}{%
\subsubsection{Simulative Outcomes}\label{simulative-outcomes}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{p}{(}\PY{l+m+mi}{13}\PY{p}{,} \PY{l+m+mf}{13.5}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{subplots\PYZus{}adjust}\PY{p}{(}\PY{n}{left}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{bottom}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{right}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,}
                    \PY{n}{top}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{wspace}\PY{o}{=}\PY{l+m+mf}{0.15}\PY{p}{,} \PY{n}{hspace}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{)}
\PY{n}{cum\PYZus{}reward} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{n}{N}\PY{p}{)}\PY{p}{)}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{e} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{epsilon}\PY{p}{)}\PY{p}{:}
    \PY{n}{epm\PYZus{}reward} \PY{o}{=} \PY{p}{[}\PY{p}{]}  \PY{c+c1}{\PYZsh{} The aggreagte reward of each experiment}
    \PY{k}{for} \PY{n}{times} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{EPS}\PY{p}{)}\PY{p}{:}
        \PY{n}{bandit} \PY{o}{=} \PY{n}{Bandit1}\PY{p}{(}\PY{p}{)}
        \PY{n}{outcome} \PY{o}{=} \PY{n}{experiment1}\PY{p}{(}\PY{n}{bandit}\PY{p}{,} \PY{n}{e}\PY{p}{)}
        \PY{n}{epm\PYZus{}reward}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{outcome}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{n}{cum\PYZus{}reward}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{outcome}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
    \PY{n}{epm\PYZus{}reward} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{epm\PYZus{}reward}\PY{p}{)}
    \PY{n}{avg} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{average}\PY{p}{(}\PY{n}{epm\PYZus{}reward}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Plot a bar chart to illustrate the aggregate reward of each experiment:}
    \PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{EPS}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{y} \PY{o}{=} \PY{n}{epm\PYZus{}reward}
    \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{bar}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Execute rounds}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Aggregate rewards}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Eps = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{, Avg = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{e}\PY{p}{,} \PY{n}{avg}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mi}{3500}\PY{p}{,} \PY{l+m+mi}{5000}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot the average aggregate reward at each time slot}
\PY{n}{cum\PYZus{}reward} \PY{o}{/}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{n}{EPS}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Time Slot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Average Aggregate Reward}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Average Aggregate Reward at each Time Slot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{cum\PYZus{}reward}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{eps = 0.2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{cum\PYZus{}reward}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{eps = 0.4}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{cum\PYZus{}reward}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{eps = 0.6}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{cum\PYZus{}reward}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{eps = 0.8}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mf}{0.6}\PY{p}{,} \PY{l+m+mf}{0.8}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project Report_files/Project Report_8_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{regret-analysis}{%
\subsubsection{Regret Analysis}\label{regret-analysis}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n}{epsilon}\PY{p}{:}
    \PY{n}{regrets} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{times} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{EPS}\PY{p}{)}\PY{p}{:}
        \PY{n}{bandit} \PY{o}{=} \PY{n}{Bandit1}\PY{p}{(}\PY{p}{)}
        \PY{n}{outcome} \PY{o}{=} \PY{n}{experiment1}\PY{p}{(}\PY{n}{bandit}\PY{p}{,} \PY{n}{e}\PY{p}{)}
        \PY{n}{regrets}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{outcome}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
    \PY{n}{gap} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{regrets}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Compute the mean and variance of 200 regrets}
    \PY{n}{Mean} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{gap}\PY{p}{)}
    \PY{n}{Var} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{var}\PY{p}{(}\PY{n}{gap}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{When epsilon = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{:}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{e}\PY{p}{)}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Mean = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{Mean}\PY{p}{)}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Var = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{Var}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
When epsilon = 0.2:
Mean = 202.465
Var = 1329.538775

When epsilon = 0.4:
Mean = 400.28
Var = 1370.7015999999996

When epsilon = 0.6:
Mean = 603.955
Var = 1201.0829749999998

When epsilon = 0.8:
Mean = 799.3
Var = 1439.17

    \end{Verbatim}

    \hypertarget{ucb-algorithm}{%
\subsection{UCB Algorithm}\label{ucb-algorithm}}

    \hypertarget{implementation}{%
\subsubsection{Implementation}\label{implementation}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{C} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{]}

\PY{k}{def} \PY{n+nf}{experiment2}\PY{p}{(}\PY{n}{bandit}\PY{p}{,} \PY{n}{c}\PY{p}{)}\PY{p}{:}
    \PY{n+nb}{sum} \PY{o}{=} \PY{l+m+mi}{0}  \PY{c+c1}{\PYZsh{} The aggreagte reward of a certain experiment}
    \PY{n}{tmp\PYZus{}cum\PYZus{}reward} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}  \PY{c+c1}{\PYZsh{} The average cumulative reward at each time slot}
    \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} Initialization}
        \PY{n}{arm} \PY{o}{=} \PY{n}{t}
        \PY{n}{reward} \PY{o}{=} \PY{n}{bandit}\PY{o}{.}\PY{n}{reward}\PY{p}{(}\PY{n}{arm}\PY{p}{)}
        \PY{n}{bandit}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{n}{arm}\PY{p}{,} \PY{n}{reward}\PY{p}{)}
        \PY{n+nb}{sum} \PY{o}{+}\PY{o}{=} \PY{n}{reward}
        \PY{n}{tmp\PYZus{}cum\PYZus{}reward}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{n}{t}\PY{o}{*}\PY{n}{tmp\PYZus{}cum\PYZus{}reward}\PY{p}{[}\PY{n}{t}\PY{p}{]}\PY{o}{+}\PY{n}{reward}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{t}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
    \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{N}\PY{p}{)}\PY{p}{:}
        \PY{n}{arm} \PY{o}{=} \PY{n}{bandit}\PY{o}{.}\PY{n}{UCB}\PY{p}{(}\PY{n}{c}\PY{p}{,} \PY{n}{t}\PY{p}{)}
        \PY{n}{reward} \PY{o}{=} \PY{n}{bandit}\PY{o}{.}\PY{n}{reward}\PY{p}{(}\PY{n}{arm}\PY{p}{)}
        \PY{n}{bandit}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{n}{arm}\PY{p}{,} \PY{n}{reward}\PY{p}{)}
        \PY{n+nb}{sum} \PY{o}{+}\PY{o}{=} \PY{n}{reward}
        \PY{n}{tmp\PYZus{}cum\PYZus{}reward}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{n}{t}\PY{o}{*}\PY{n}{tmp\PYZus{}cum\PYZus{}reward}\PY{p}{[}\PY{n}{t}\PY{p}{]}\PY{o}{+}\PY{n}{reward}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{t}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{p}{[}\PY{n+nb}{sum}\PY{p}{,} \PY{l+m+mi}{4800}\PY{o}{\PYZhy{}}\PY{n+nb}{sum}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{tmp\PYZus{}cum\PYZus{}reward}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{simulative-outcomes}{%
\subsubsection{Simulative Outcomes}\label{simulative-outcomes}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{53}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{13}\PY{p}{,}\PY{l+m+mi}{9}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{subplots\PYZus{}adjust}\PY{p}{(}\PY{n}{left}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{bottom}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{right}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,}
                    \PY{n}{top}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{wspace}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{hspace}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{)}
\PY{n}{cum\PYZus{}reward} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{N}\PY{p}{)}\PY{p}{)}

\PY{k}{for} \PY{n}{i}\PY{p}{,}\PY{n}{c} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{C}\PY{p}{)}\PY{p}{:}
    \PY{n}{epm\PYZus{}reward} \PY{o}{=} \PY{p}{[}\PY{p}{]}  \PY{c+c1}{\PYZsh{} The aggregate reward of each experiment}
    \PY{k}{for} \PY{n}{times} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{EPS}\PY{p}{)}\PY{p}{:}
        \PY{n}{bandit} \PY{o}{=} \PY{n}{Bandit1}\PY{p}{(}\PY{p}{)}
        \PY{n}{outcome} \PY{o}{=} \PY{n}{experiment2}\PY{p}{(}\PY{n}{bandit}\PY{p}{,} \PY{n}{c}\PY{p}{)}
        \PY{n}{epm\PYZus{}reward}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{outcome}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{n}{cum\PYZus{}reward}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{outcome}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
    \PY{n}{epm\PYZus{}reward} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{epm\PYZus{}reward}\PY{p}{)}
    \PY{n}{avg} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{average}\PY{p}{(}\PY{n}{epm\PYZus{}reward}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Plot a bar chart to illustrate the aggregate reward of each experiment:}
    \PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{EPS}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{y} \PY{o}{=} \PY{n}{epm\PYZus{}reward}
    \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{bar}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Execute rounds}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Aggregate rewards}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{c = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{, Avg = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{c}\PY{p}{,} \PY{n}{avg}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mi}{3500}\PY{p}{,} \PY{l+m+mi}{5000}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot the average aggregate reward of each time slot}
\PY{n}{cum\PYZus{}reward} \PY{o}{/}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{n}{EPS}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Time Slot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Average Aggregate Reward}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Average Aggregate Reward at each Time Slot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{cum\PYZus{}reward}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{c = 2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{cum\PYZus{}reward}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{c = 6}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{cum\PYZus{}reward}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{c = 9}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mf}{0.6}\PY{p}{,} \PY{l+m+mf}{0.8}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project Report_files/Project Report_15_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{regret-analysis}{%
\subsubsection{Regret Analysis}\label{regret-analysis}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{54}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{for} \PY{n}{c} \PY{o+ow}{in} \PY{n}{C}\PY{p}{:}
    \PY{n}{regrets} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{times} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{EPS}\PY{p}{)}\PY{p}{:}
        \PY{n}{bandit} \PY{o}{=} \PY{n}{Bandit1}\PY{p}{(}\PY{p}{)}
        \PY{n}{outcome} \PY{o}{=} \PY{n}{experiment2}\PY{p}{(}\PY{n}{bandit}\PY{p}{,} \PY{n}{c}\PY{p}{)}
        \PY{n}{regrets}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{outcome}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
    \PY{n}{gap} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{regrets}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Compute the mean and variance of 200 regrets}
    \PY{n}{Mean} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{gap}\PY{p}{)}
    \PY{n}{Var} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{var}\PY{p}{(}\PY{n}{gap}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{When c = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{:}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{c}\PY{p}{)}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Mean = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{Mean}\PY{p}{)}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Var = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{Var}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
When c = 2:
Mean = 258.55
Var = 1184.9475

When c = 6:
Mean = 662.0
Var = 1166.33

When c = 9:
Mean = 773.965
Var = 1113.153775

    \end{Verbatim}

    \hypertarget{thompson-sampling-algorithm}{%
\subsection{Thompson Sampling
Algorithm}\label{thompson-sampling-algorithm}}

    \hypertarget{implementation}{%
\subsubsection{Implementation}\label{implementation}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{61}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Alpha} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{601}\PY{p}{,} \PY{l+m+mi}{401}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{n}{Beta} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{401}\PY{p}{,} \PY{l+m+mi}{601}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{]}\PY{p}{)}

\PY{k}{class} \PY{n+nc}{Bandit2}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{alpha}\PY{p}{,} \PY{n}{beta}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{prob} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{prob}\PY{p}{)}  \PY{c+c1}{\PYZsh{} The true rate of reward of each arm}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{alpha} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{alpha}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{beta} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{beta}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{reward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{arm}\PY{p}{)}\PY{p}{:}
        \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Generate the reward by Bernoulli trial\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{k}{return} \PY{n}{random}\PY{o}{.}\PY{n}{binomial}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{prob}\PY{p}{[}\PY{n}{arm}\PY{p}{]}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{sample}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Sample estimated rewards from Beta distributions\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{n}{est\PYZus{}values} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{beta}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{alpha}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{beta}\PY{p}{)}
        \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{est\PYZus{}values}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{update}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{arm}\PY{p}{,} \PY{n}{R}\PY{p}{)}\PY{p}{:}
        \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Update the number of being selected and the estimated reward of the arm\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{alpha}\PY{p}{[}\PY{n}{arm}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{R}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{beta}\PY{p}{[}\PY{n}{arm}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{R}

\PY{k}{def} \PY{n+nf}{experiment3}\PY{p}{(}\PY{n}{bandit}\PY{p}{)}\PY{p}{:}
    \PY{n+nb}{sum} \PY{o}{=} \PY{l+m+mi}{0}  \PY{c+c1}{\PYZsh{} The aggreagte reward of a certain experiment}
    \PY{n}{tmp\PYZus{}cum\PYZus{}reward} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}  \PY{c+c1}{\PYZsh{} The average cumulative reward at each time slot}
    \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{N}\PY{p}{)}\PY{p}{:}
        \PY{n}{arm} \PY{o}{=} \PY{n}{bandit}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{p}{)}
        \PY{n}{reward} \PY{o}{=} \PY{n}{bandit}\PY{o}{.}\PY{n}{reward}\PY{p}{(}\PY{n}{arm}\PY{p}{)}
        \PY{n}{bandit}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{n}{arm}\PY{p}{,} \PY{n}{reward}\PY{p}{)}
        \PY{n+nb}{sum} \PY{o}{+}\PY{o}{=} \PY{n}{reward}
        \PY{n}{tmp\PYZus{}cum\PYZus{}reward}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{n}{t}\PY{o}{*}\PY{n}{tmp\PYZus{}cum\PYZus{}reward}\PY{p}{[}\PY{n}{t}\PY{p}{]}\PY{o}{+}\PY{n}{reward}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{t}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
    \PY{k}{return} \PY{p}{[}\PY{n+nb}{sum}\PY{p}{,} \PY{l+m+mi}{4800}\PY{o}{\PYZhy{}}\PY{n+nb}{sum}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{tmp\PYZus{}cum\PYZus{}reward}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{simulative-outcomes}{%
\subsubsection{Simulative Outcomes}\label{simulative-outcomes}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{47}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{13}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{subplots\PYZus{}adjust}\PY{p}{(}\PY{n}{left}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{bottom}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{right}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,}
                    \PY{n}{top}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{wspace}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{hspace}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{)}
\PY{n}{cum\PYZus{}reward} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{N}\PY{p}{)}\PY{p}{)}

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{:}
    \PY{n}{epm\PYZus{}reward} \PY{o}{=} \PY{p}{[}\PY{p}{]}  \PY{c+c1}{\PYZsh{} The aggregate reward of each experiment}
    \PY{k}{for} \PY{n}{times} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{EPS}\PY{p}{)}\PY{p}{:}
        \PY{n}{alpha} \PY{o}{=} \PY{n}{Alpha}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
        \PY{n}{beta} \PY{o}{=} \PY{n}{Beta}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
        \PY{n}{bandit} \PY{o}{=} \PY{n}{Bandit2}\PY{p}{(}\PY{n}{alpha}\PY{p}{,} \PY{n}{beta}\PY{p}{)}
        \PY{n}{outcome} \PY{o}{=} \PY{n}{experiment3}\PY{p}{(}\PY{n}{bandit}\PY{p}{)}
        \PY{n}{epm\PYZus{}reward}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{outcome}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{n}{cum\PYZus{}reward}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{outcome}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
    \PY{n}{epm\PYZus{}reward} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{epm\PYZus{}reward}\PY{p}{)}
    \PY{n}{avg} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{average}\PY{p}{(}\PY{n}{epm\PYZus{}reward}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Plot a bar chart to illustrate the aggregate reward of each experiment:}
    \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{EPS}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{y} \PY{o}{=} \PY{n}{epm\PYZus{}reward}
    \PY{n}{plt}\PY{o}{.}\PY{n}{bar}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Execute rounds}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Aggregate rewards}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{(α1, β1) = (}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{,}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{), (α2, β2) = (}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{,}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{), (α3, β3) = (}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{,}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{)}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Avg = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}
        \PY{n}{Alpha}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{Beta}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{Alpha}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{Beta}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{Alpha}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{Beta}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{avg}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mi}{4000}\PY{p}{,} \PY{l+m+mi}{5500}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot the average aggregate reward of each time slot}
\PY{n}{cum\PYZus{}reward} \PY{o}{/}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{n}{EPS}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Time Slot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Average Aggregate Reward}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Average Aggregate Reward at each Time Slot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{cum\PYZus{}reward}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{α = (1, 1, 1)}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{β = (1, 1, 1)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{cum\PYZus{}reward}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{α = (601, 401, 2)}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{β = (401, 601, 3)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mf}{0.6}\PY{p}{,} \PY{l+m+mf}{0.8}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project Report_files/Project Report_22_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{regret-analysis}{%
\subsubsection{Regret Analysis}\label{regret-analysis}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{64}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{:}
    \PY{n}{regrets} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{times} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{EPS}\PY{p}{)}\PY{p}{:}
        \PY{n}{alpha} \PY{o}{=} \PY{n}{Alpha}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
        \PY{n}{beta} \PY{o}{=} \PY{n}{Beta}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
        \PY{n}{bandit} \PY{o}{=} \PY{n}{Bandit2}\PY{p}{(}\PY{n}{alpha}\PY{p}{,} \PY{n}{beta}\PY{p}{)}
        \PY{n}{outcome} \PY{o}{=} \PY{n}{experiment3}\PY{p}{(}\PY{n}{bandit}\PY{p}{)}
        \PY{n}{regrets}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{outcome}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
    \PY{n}{gap} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{regrets}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Compute the mean and variance of 200 regrets}
    \PY{n}{Mean} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{gap}\PY{p}{)}
    \PY{n}{Var} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{var}\PY{p}{(}\PY{n}{gap}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{When (alpha\PYZus{}1, beta\PYZus{}1) = (}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{,}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{), (alpha\PYZus{}2, beta\PYZus{}2) = (}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{,}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{), (alpha\PYZus{}3, beta\PYZus{}3) = (}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{,}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{):}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}
        \PY{n}{Alpha}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{Beta}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{Alpha}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{Beta}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{Alpha}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{Beta}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Mean = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{Mean}\PY{p}{)}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Var = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{Var}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
When (alpha\_1, beta\_1) = (1,1), (alpha\_2, beta\_2) = (1,1), (alpha\_3, beta\_3) =
(1,1):
Mean = 13.675
Var = 867.499375

When (alpha\_1, beta\_1) = (601,401), (alpha\_2, beta\_2) = (401,601), (alpha\_3,
beta\_3) = (2,3):
Mean = 14.855
Var = 989.6039749999999

    \end{Verbatim}

    \hypertarget{comparison-of-different-algorithms}{%
\section{Comparison of Different
Algorithms}\label{comparison-of-different-algorithms}}

    In this section, we compare the regrets of three algorithms and
determine which is the best.

    \hypertarget{summary-of-algorithm-performances}{%
\subsection{Summary of Algorithm
Performances}\label{summary-of-algorithm-performances}}

Here are some tables demonstrating the mean and variance of regrets over
200 experiments.

    \hypertarget{epsilon-greedy}{%
\subsubsection{\texorpdfstring{\(\epsilon\)-greedy}{\textbackslash epsilon-greedy}}\label{epsilon-greedy}}

\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline 
        \(\epsilon\) & 0.2 & 0.4 & 0.6 & 0.8 \\
        \hline 
        Mean & 202.465 & 400.28 & 603.955 & 799.3 \\ 
        \hline
        Variance & 1329.53 & 1370.70 & 1201.08 & 1439.17 \\
        \hline
    \end{tabular} 
\end{table}

\hypertarget{ucb}{%
\subsubsection{UCB}\label{ucb}}

\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline 
        \(c\) & 2 & 6 & 9 \\
        \hline 
        Mean & 258.55 & 662.0 & 773.965 \\ 
        \hline
        Variance & 1184.94 & 1166.33 & 1113.15 \\
        \hline
    \end{tabular} 
\end{table}

\hypertarget{thompson-sampling}{%
\subsubsection{Thompson Sampling}\label{thompson-sampling}}

\begin{longtable}[]{@{}ccc@{}}
\toprule
\begin{minipage}[b]{0.08\columnwidth}\centering
\((\alpha,\beta)\)\strut
\end{minipage} & \begin{minipage}[b]{0.39\columnwidth}\centering
\((\alpha_1,\beta_1)=(1,1),(\alpha_2,\beta_2)=(1,1),(\alpha_3,\beta_3)=(1,1)\)\strut
\end{minipage} & \begin{minipage}[b]{0.44\columnwidth}\centering
\((\alpha_1,\beta_1)=(601,401),(\alpha_2,\beta_2)=(401,601),(\alpha_3,\beta_3)=(2,3)\)\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.08\columnwidth}\centering
Mean\strut
\end{minipage} & \begin{minipage}[t]{0.39\columnwidth}\centering
13.675\strut
\end{minipage} & \begin{minipage}[t]{0.44\columnwidth}\centering
14.855\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.08\columnwidth}\centering
Variance\strut
\end{minipage} & \begin{minipage}[t]{0.39\columnwidth}\centering
867.49\strut
\end{minipage} & \begin{minipage}[t]{0.44\columnwidth}\centering
989.60\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    \hypertarget{analysis}{%
\subsection{Analysis}\label{analysis}}

We focus on both mean and variance of the regrets, because the accuracy
and stability are the two most important evaluation indices of the
performances of algorithms.

    \hypertarget{accuracy}{%
\subsubsection{Accuracy}\label{accuracy}}

Thompson sampling algorithm has the highest accuracy, with the mean of
regret only around 13-15. The means of other two algorithms are both
over 200, with more regrets as their hyperparameters increase.

    \hypertarget{stability}{%
\subsubsection{Stability}\label{stability}}

From Thompson sampling, to UCB, to \(\epsilon\)-greedy, the variance
gradually increases, meaning that the stability gradually decreases.
Therefore, Thompson sampling is relatively the most stable algorithm,
while the other two algorithms are less stable.

    \hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

Comprehensively consider both accuracy and stability, Thompson sampling
is the optimal algorithm of classic multi-armed bandit, since it is the
most accurate and most stable.

    \hypertarget{discussion-of-the-impacts-of-different-parameters}{%
\section{Discussion of the Impacts of Different
Parameters}\label{discussion-of-the-impacts-of-different-parameters}}

    Since all the experiments are based on the already given parameters, we
need to further explore the impacts of their respective parameters and
find the optimal choices if possible.

\textbf{By the way, since the exploration-exploitation trade-off is
highly correlated to the impacts of hyperparameters, my further
understanding towards the trade-off problem will be included in either
\emph{Analysis} or \emph{Conclusion} part, or both. Honestly, it is
complicated to discuss it separately in another section.}

The general idea of exploration-exploitation trade-off is:

\begin{itemize}
\item
  Both exploration and exploitation have pros and cons, we need to find
  an appropriate ratio to determine how much should we explore and
  exploit, to maximize their advantages and minimize their
  disadvantages.
\item
  As for exploration, sometimes the potentially optimal arm has not been
  observed, so appropriately exploring provides more chances to find the
  optimal arm. However, if the proportion of exploration is too much,
  our strategy approximately turns out to be randomization, so the
  potentially optimal arm may not be pulled many times. Exploration is
  similar to stake, because every time we decide to explore, there are
  two possible outcomes: either being closed to the potential
  optimality, or running into non-optimal arm and hence causing
  deadweight loss.
\item
  As for exploitation, sometimes the locally optimal solution is exactly
  the globally optimal solution, so repeatedly pulling the locally
  optimal arm leads to optimal reward. However, sometimes the locally
  optimal solution is not the globally optimal solution, if we exploit
  too much, we may be trapped in the local optimality and miss the
  potential global optimality.
\item
  The trade-off of exploration-exploitation, is exactly how to firstly
  find a local optimality, and then try to escape the bondage of local
  optimality and transit to global optimality.
\end{itemize}

More detailed comprehension of exploration-exploitation trade-off will
be further discussed in the following part.

    \hypertarget{epsilon-greedy}{%
\subsection{\texorpdfstring{\(\epsilon\)-greedy}{\textbackslash epsilon-greedy}}\label{epsilon-greedy}}

    \hypertarget{find-the-optimal-epsilon-by-simulation}{%
\subsubsection{\texorpdfstring{Find the Optimal \(\epsilon\) by
Simulation}{Find the Optimal \textbackslash epsilon by Simulation}}\label{find-the-optimal-epsilon-by-simulation}}

We will apply different values of \(\epsilon\) to the algorithm and try
to find out which performs the best. For \(\epsilon\in[0,1]\), we take
\(\epsilon=0.05k\), where \(k=0,1,\cdots,20\), to compare the aggregate
rewards over \(6000\) time slots given different \(\epsilon\)'s.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{epsilon} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.05}\PY{o}{*}\PY{n}{k} \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{21}\PY{p}{)}\PY{p}{]}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{p}{(}\PY{l+m+mf}{6.5}\PY{p}{,} \PY{l+m+mf}{4.5}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} The average reward at each time slot for different epsilons}
\PY{n}{agg\PYZus{}avg\PYZus{}reward} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{21}\PY{p}{,} \PY{n}{N}\PY{p}{)}\PY{p}{)}

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{epsilon}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n}{e} \PY{o}{=} \PY{n}{epsilon}\PY{p}{[}\PY{n}{i}\PY{p}{]}
    \PY{k}{for} \PY{n}{times} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{EPS}\PY{p}{)}\PY{p}{:}
        \PY{n}{bandit} \PY{o}{=} \PY{n}{Bandit}\PY{p}{(}\PY{p}{)}
        \PY{n}{outcome} \PY{o}{=} \PY{n}{experiment1}\PY{p}{(}\PY{n}{bandit}\PY{p}{,} \PY{n}{e}\PY{p}{)}
        \PY{n}{agg\PYZus{}avg\PYZus{}reward}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{outcome}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
\PY{c+c1}{\PYZsh{} Compute the average cumulative average reward over 200 experiments:}
\PY{n}{agg\PYZus{}avg\PYZus{}reward} \PY{o}{/}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{n}{EPS}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Visualization:}
\PY{n}{x}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{epsilon}\PY{p}{)}
\PY{n}{y} \PY{o}{=} \PY{n}{agg\PYZus{}avg\PYZus{}reward}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{N}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epsilon}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Average Aggregate Reward}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Average Aggregate Reward given Epsilon}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mf}{0.6}\PY{p}{,} \PY{l+m+mf}{0.85}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project Report_files/Project Report_37_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{analysis}{%
\subsubsection{Analysis}\label{analysis}}

By definition, \(\epsilon\) is the probability of randomly choosing an
arm to pull at each time slot, which is the process of exploration. In
fact, it represents our consideration towards the trade-off of
exploitation and exploration, since by simple greedy strategy we should
pull the arm with the highest reward rate, but based on the limited
samples, we cannot take it for granted that we must pull the temporarily
best arm. It is also possible that randomly choosing another arm can
have a higher possibility to earn a reward.

Based on the curves above, we know that \(\epsilon=0\) is optimal, and
the algorithm performs worse and worse while \(\epsilon\) is increasing.
That is to say, the more exploitation, the more rewards.

However, the result is inconsistent with our intuitive judgment, because
obviously, exploring with a small probability can somehow decrease the
uncertainty of locally optimal solution by trying to avoid repeatedly
exploiting it. So we guess that the optimal parameter \(\epsilon=0\) is
only appropriate for THIS multi-armed bandit, with real rates of return
where \(\theta_1=0.8,\theta_2=0.6,\theta_3=0.5\).

Here is a more detailed explanation about the reason why \(\epsilon=0\)
is optimal:

With \(\epsilon=0\), we should exploit the best arm at each time slot.
We initialize \(\hat{\theta_1}=\hat{\theta_2}=\hat{\theta_3}=0\), so by
the operating mechanism of \(np.argmax(\hat{\theta_j})\), when all the
\(\hat{\theta_j}\)'s are the same, it will return the first index of
identical elements by default, which is the first arm.

Since \(\theta_1=0.8\) is the highest reward rate, the default outcome
of the decision is exactly the best choice by coincidence. If the reward
at this time is 1, \(\hat{\theta_1}\) will be updated to be larger so we
prefer to choose arm 1 at the following time slots. It does not matter
if the reward is 0 because \(\hat{\theta_1}\) will be updated to be 0,
which comes back to the initial status without extra influence.

Since pulling arm 1 can get a reward more probably, during the whole
process of pulling arms, in general \(\hat{\theta_1}\) will be the
largest estimation among three estimations, so we tend to pull arm 1
more than the other 2 arms and therefore the aggregate reward will be
relatively maximized.

In conclusion, because of the coincidence that
\(np.argmax(\hat{\theta_j})\) will automatically return index 1 and
pulling arm 1 has the highest probability to earn a reward, the greedy
strategy relatively performs better than those with a small probability
to randomly explore, so \(\epsilon=0\) is optimal.

However, when the parameters \(\theta_j\)'s are different, the optimal
\(\epsilon\) should change.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{draw}\PY{p}{(}\PY{n}{pic\PYZus{}name}\PY{p}{)}\PY{p}{:}
    \PY{n}{img} \PY{o}{=} \PY{n}{Image}\PY{o}{.}\PY{n}{open}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{pic\PYZus{}name} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{140}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{img}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{off}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    For example, when \((\theta_1,\theta_2,\theta_3)=(0.5,0.6,0.8)\), a
relatively general scenario, applying different \(\epsilon\)'s we have

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{draw}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Figure\PYZus{}1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project Report_files/Project Report_41_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    By the simulative outcome, the simple greedy strategy with
\(\epsilon=0\) is no longer appropriate and \(\epsilon=0.05\) seems to
be optimal.

More specifically, a closer examination of the variation of the average
aggregate reward as \(\epsilon\) varies near \(0.05\) is shown below.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{draw}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Figure\PYZus{}2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project Report_files/Project Report_43_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Therefore we can approximately determine that \(\epsilon_0=0.035\) when
\((\theta_1,\theta_2,\theta_3)=(0.5,0.6,0.8)\).

    \hypertarget{conclusion-trade-off-problem-is-discussed-simultaneously}{%
\subsubsection{Conclusion (Trade-off problem is discussed
simultaneously)}\label{conclusion-trade-off-problem-is-discussed-simultaneously}}

By the discussion above, we know that for a multi-armed bandit, there
must exist a single value \(\epsilon_0\) that can optimize the
\(\epsilon\)-greedy algorithm.

More detailed explanation about the impact of \(\epsilon\) is stated
below:

\begin{itemize}
\item
  When \(\epsilon=0\), the \(\epsilon\)-greedy algorithm degrades into a
  simple greedy algorithm. Initially the first arm will be chosen and
  after that the highest estimated arm is chosen at each time slot. The
  potentially optimal arm may not be selected.
\item
  When \(\epsilon\) increases with \(\epsilon<\epsilon_0\), the
  probability of exploration increases as well, so the optimal arm will
  be selected with increasing frequency and the non-optimal arm will be
  selected less. Therefore, obviously the aggregate reward keeps
  increasing.
\item
  The increament of the aggregate reward continues until
  \(\epsilon=\epsilon_0\), where the aggregate reward reaches its peak
  \(R_{max}\) simultaneously. The point \((\epsilon_0,R_{max})\)
  represents the best solution and its outcome in the
  exploration-exploitation trade-off. That is to say, exploration with
  probability \(\epsilon_0\) can maximize the aggregate reward of the
  multi-armed bandit.
\item
  When \(\epsilon_0<\epsilon<1\), the algorithm starts to shift in favor
  of exploration. Each arm will be randomly selected with increasing
  probability. As a result, the non-optimal arms begin to be chosen more
  frequently so the aggregate reward begins to fall.
\item
  When \(\epsilon=1\), randomization totally occupies all the decisions
  at each time slot, so exploration completely replaces exploitation. No
  matter what the estimaed reward of each arm is, they will be totally
  randomly selected, so we can no longer exploit the optimal arm.
  Therefore, the aggregate reward reaches relatively the lowest.
\end{itemize}

In conclusion, the selection of hyperparameter \(\epsilon\) reflects the
trade-off of exploitation and exploration: to what extend should we
exploit the arm with the maximum estimated reward, or explore a random
arm to make it more possible to find the potentially optimal arm?

As for this algorithm, the trade-off is simple, that is, we either
choose to explore or to exploit, which is \textbf{binary opposition}
relationship.

    \hypertarget{ucb}{%
\subsection{UCB}\label{ucb}}

    \hypertarget{find-the-optimal-c-by-simulation}{%
\subsubsection{\texorpdfstring{Find the Optimal \(c\) by
Simulation}{Find the Optimal c by Simulation}}\label{find-the-optimal-c-by-simulation}}

We will apply different values of \(c\), the confidence value, to the
algorithm and try to find out which performs the best. Firstly, we take
\(c=2k\), where \(k=0,1,\cdots,20\). The behaviour of the UCB algorithm,
over a range of confidence values, is shown below.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{C} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{k} \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{21}\PY{p}{)}\PY{p}{]}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} The average reward at each time slot for different epsilons}
\PY{n}{agg\PYZus{}avg\PYZus{}reward} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{21}\PY{p}{,} \PY{n}{N}\PY{p}{)}\PY{p}{)}

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{C}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n}{c}\PY{o}{=}\PY{n}{C}\PY{p}{[}\PY{n}{i}\PY{p}{]}
    \PY{k}{for} \PY{n}{times} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{EPS}\PY{p}{)}\PY{p}{:}
        \PY{n}{bandit} \PY{o}{=} \PY{n}{Bandit}\PY{p}{(}\PY{p}{)}
        \PY{n}{outcome} \PY{o}{=} \PY{n}{experiment2}\PY{p}{(}\PY{n}{bandit}\PY{p}{,} \PY{n}{c}\PY{p}{)}
        \PY{n}{agg\PYZus{}avg\PYZus{}reward}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{outcome}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
\PY{c+c1}{\PYZsh{} Compute the average cumulative average reward over 200 experiments:}
\PY{n}{agg\PYZus{}avg\PYZus{}reward} \PY{o}{/}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{n}{EPS}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Visualization:}
\PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{C}\PY{p}{)}
\PY{n}{y} \PY{o}{=} \PY{n}{agg\PYZus{}avg\PYZus{}reward}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{N}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Confidence value}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Average Aggregate Reward}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Average Aggregate Reward given C}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mf}{0.6}\PY{p}{,} \PY{l+m+mf}{0.8}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project Report_files/Project Report_48_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Since the average aggregate reward drops dramatically when \(c>2\), in
order to better determine the optimal confidence value, we need to
shrink the range of it to \([0.0,2.0]\) for a closer examination. As we
take \(c=0.1k\), where \(k=0,1,\cdots,20\), the average aggregate reward
as the confidence level is increased is shown below.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{draw}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Figure\PYZus{}3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project Report_files/Project Report_50_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Here it can be seen that the average aggregate reward does increase,
from having a confidence parameter of zero up to a value of
approximately \(c_0=0.3\), after which it drops gradually. So a small
degree of exploration is required to get the best results.

    \hypertarget{analysis-trade-off-problem-is-discussed-simultaneously}{%
\subsubsection{Analysis (Trade-off problem is discussed
simultaneously)}\label{analysis-trade-off-problem-is-discussed-simultaneously}}

The key decisive factor
\(\hat{\theta}(j)+c\cdot\sqrt{\frac{2\ln(t)}{\text{count}(j)}}\) is a
combination of exploitation and exploration. The exploitative factor is
\(\hat{\theta}(j)\) and the explorative factor is
\(\sqrt{\frac{2\ln(t)}{\text{count}(j)}}\), controlled by the
hyper-parameter \(c\), which is called confidence value. The meaning of
\(\hat{\theta}(j)\) is obvious, so we mainly discuss the meaning of
\(\sqrt{\frac{2\ln(t)}{\text{count}(j)}}\).

Mathmatically, by Hoeffding bound,
\(\hat{\theta}(j)+\sqrt{\frac{2\ln(t)}{\text{count}(j)}}\) is the upper
bound of \(1-2t^{-4}\) confidence interval of \(\theta_j\), and
therefore the hyperparameter \(c\) controls the size of confidence
interval. The size of confidence interval also reflects our uncertainty
towards the estimated rate of reward.

The concept of ``uncertainty'' is how confident we are in the estimated
reward on a certain arm. If it is large, we may be confused about
whether its temporary estimated reward is accurate, while if it is
small, we can believe its accuracy to more extend. By the algorithm, the
larger uncertainty an arm has, the larger probability it will be
selected, since the upper bound of confidence interval is larger,
indicating a higher probability that its potential rate of reward can be
larger.

In fact, \(\sqrt{\frac{2\ln(t)}{\text{count}(j)}}\) is a measurement of
the uncertainty, and there are two deterministic factors,
\(\sqrt{\ln(t)}\) and \(\sqrt{\text{count}(j)}\).

If an arm is not selected very often, then \(\text{count}(j)\) will be
small, consequently the uncertainty term will be large, making this arm
more likely to be selected. Each time the arm is selected,
\(\text{count}(j)\) increases so we will be more confident about its
estimated reward no matter whether its present output is 0 or 1, making
it less likely to be selected.

Since as the time passes \(\ln(t)\) increases as well, if an arm is not
selected for a while, its uncertainty will increase because as the
proportion of \(\text{count}(j)\) over \(t\) time slots becomes smaller,
we are more unconfident about the estimated reward of the arm.

    \hypertarget{conclusion-trade-off-problem-is-discussed-simultaneously}{%
\subsubsection{Conclusion (Trade-off problem is discussed
simultaneously)}\label{conclusion-trade-off-problem-is-discussed-simultaneously}}

Based the analysis above, we can know that \(c\), the confidence value,
controls the weight of uncertainty in the exploration-exploitation
trade-off. The larger \(c\) is, the more weight the exploration term
has. By monotonicity, there must exist a single value \(c_0\) that can
optimize the UCB algorithm.

More detailed explanation about the impact of \(\epsilon\) is stated
below:

\begin{itemize}
\item
  When \(c<c_0\), the selective decision is mainly determined by the
  estimaed values of arms. As the confidence value increases,
  uncertainty is considered more in the decision, so the arm with higher
  uncertainty is more likely to be selected so that its uncertainty can
  be sharply decreased, which makes it clearer for the estimated value.
  Also, the upper bound of confidence interval is appropriately
  enlarged, which gives a reasonable prediction of the true rate of
  reward, so the arm with maximal upper confidence bound can be regarded
  as the local optimum. Therefore, the aggregate reward increases
  simultaneously as \(c\) increases.
\item
  When \(c=c_0\), the exploration-exploitation trade-off is highly
  optimized. Selections in this scenario can not only consider the
  estimated value to maximize the reward, but also consider the
  uncertainty to decrease it by selecting the corresponding arm. Also,
  the prediction of the upper bound is the most appropriate. Therefore
  the point \((c_0,R_{max})\) is at the peak of the reward curve.
\item
  When \(c>c_0\), the uncertainty is over-considered. Therefore, our
  strategy focuses more on how to reduce the uncertainty but relatively
  ignores the contributions of the estimated rewards, which can help us
  find the optimal arm. Therefore, the aggregate reward continuously
  decreases as \(c\) increases.
\end{itemize}

The strategy of UCB algorithm is highly dependent on the combination of
exploration and exploitation, i.e., the temporary upper confidence bound
with proper enlargement
\(\hat{\theta}(j)+c\sqrt{\frac{2\ln(t)}{\text{count}(j)}}\), which is
more like \textbf{cooperation and mutual benefits}.

In conclusion, the selection of hyperparameter \(c\), the confidence
value, represents the relative weight of exploration part and
exploitation part. Proper selection can highly balance the contribution
of either part, which maximizes the joint contribution.

    \hypertarget{thompson-sampling}{%
\subsection{Thompson Sampling}\label{thompson-sampling}}

    \hypertarget{detailed-variance-of-parameters}{%
\subsubsection{Detailed Variance of
Parameters}\label{detailed-variance-of-parameters}}

It is difficult to give an intuitive explanation of the impacts of
\(\alpha_j\) and \(\beta_j\), so we need draw several plots to show the
changes of the PDFs of Beta distributions, where we can find out the
sample strategy of TS.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Alpha} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{Beta} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}

\PY{k}{class} \PY{n+nc}{ARM}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{p}\PY{p}{,}\PY{n+nb}{id}\PY{p}{,}\PY{n}{alpha}\PY{p}{,}\PY{n}{beta}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{a}\PY{o}{=}\PY{n}{alpha}\PY{p}{[}\PY{n+nb}{id}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b}\PY{o}{=}\PY{n}{beta}\PY{p}{[}\PY{n+nb}{id}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{p}\PY{o}{=}\PY{n}{p}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{total}\PY{o}{=}\PY{l+m+mi}{0}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{success}\PY{o}{=}\PY{l+m+mi}{0}

    \PY{k}{def} \PY{n+nf}{reward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Generate the reward by Bernoulli trial\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{k}{return} \PY{n}{random}\PY{o}{.}\PY{n}{binomial}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{p}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{sample}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Generate the estimated reward through Beta distribution\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{k}{return} \PY{n}{random}\PY{o}{.}\PY{n}{beta}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{a}\PY{p}{,}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{update}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{R}\PY{p}{)}\PY{p}{:}
        \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Update the parameters of Beta distribution and the estimated reward of the arm\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{total}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{a}\PY{o}{+}\PY{o}{=}\PY{n}{R}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{R}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{success}\PY{o}{+}\PY{o}{=}\PY{n}{R}

\PY{c+c1}{\PYZsh{} Generate 3 arms:}
\PY{n}{arms}\PY{o}{=}\PY{p}{[}\PY{n}{ARM}\PY{p}{(}\PY{n}{prob}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,}\PY{n}{i}\PY{p}{,}\PY{n}{Alpha}\PY{p}{,}\PY{n}{Beta}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{]}

\PY{c+c1}{\PYZsh{} A series of \PYZdq{}milestones\PYZdq{} where we output the PDFs when the time slot reaches each of them:}
\PY{n}{trails}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{,}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+m+mi}{500}\PY{p}{,}\PY{l+m+mi}{1000}\PY{p}{,}\PY{l+m+mi}{3000}\PY{p}{,}\PY{l+m+mi}{6000}\PY{p}{]}

\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{p}{(}\PY{l+m+mf}{11.0}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{n}{beta} \PY{o}{=} \PY{n}{stats}\PY{o}{.}\PY{n}{beta}
\PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{draw}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{Arm\PYZus{}total} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{arm}\PY{o}{.}\PY{n}{total} \PY{k}{for} \PY{n}{arm} \PY{o+ow}{in} \PY{n}{arms}\PY{p}{]}\PY{p}{)}\PY{p}{)}

    \PY{n}{colors} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{green}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{red}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{blue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
    \PY{n}{c\PYZus{}index} \PY{o}{=} \PY{l+m+mi}{0}

    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{:}
        \PY{n}{c} \PY{o}{=} \PY{n}{colors}\PY{p}{[}\PY{n}{c\PYZus{}index}\PY{p}{]}
        \PY{n}{y} \PY{o}{=} \PY{n}{beta}\PY{p}{(}\PY{n}{arms}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{a}\PY{p}{,} \PY{n}{arms}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{b}\PY{p}{)}
        \PY{n}{p} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{o}{.}\PY{n}{pdf}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{,} \PY{n}{lw}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{n}{c}\PY{p}{,}
                     \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{/}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{(}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{,}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{)}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{arms}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{success}\PY{p}{,} \PY{n}{arms}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{total}\PY{p}{,} \PY{n}{arms}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{a}\PY{p}{,}\PY{n}{arms}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{b}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{o}{.}\PY{n}{pdf}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{n}{c}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{vlines}\PY{p}{(}\PY{n}{prob}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{y}\PY{o}{.}\PY{n}{pdf}\PY{p}{(}
            \PY{n}{prob}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{colors}\PY{o}{=}\PY{n}{c}\PY{p}{,} \PY{n}{linestyles}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{lw}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{autoscale}\PY{p}{(}\PY{n}{tight}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{True}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{PDFs after }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ Time Slots}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{Arm\PYZus{}total}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{autoscale}\PY{p}{(}\PY{n}{tight}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        \PY{n}{c\PYZus{}index} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}

\PY{k}{def} \PY{n+nf}{TS}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{trail}\PY{o}{=}\PY{l+m+mi}{0} \PY{c+c1}{\PYZsh{} The index of \PYZdq{}milestones\PYZdq{}}
    \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{N}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} If the time slot reaches a \PYZdq{}milestone\PYZdq{}, then plot the PDFs:}
        \PY{k}{if} \PY{p}{(}\PY{n}{t}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{==} \PY{n}{trails}\PY{p}{[}\PY{n}{trail}\PY{p}{]}\PY{p}{:}
            \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{trail}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{draw}\PY{p}{(}\PY{p}{)}
            \PY{n}{trail} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
            \PY{n}{plt}\PY{o}{.}\PY{n}{autoscale}\PY{p}{(}\PY{n}{tight}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Generate the estimated rewards of 3 arms:}
        \PY{n}{est}\PY{o}{=}\PY{p}{[}\PY{n}{arm}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{arm} \PY{o+ow}{in} \PY{n}{arms}\PY{p}{]}

        \PY{c+c1}{\PYZsh{} Find the arm with the maximum estimated reward:}
        \PY{n}{Arm\PYZus{}index}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{est}\PY{p}{)}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Generate the reward and update its parameters:}
        \PY{n}{reward}\PY{o}{=}\PY{n}{arms}\PY{p}{[}\PY{n}{Arm\PYZus{}index}\PY{p}{]}\PY{o}{.}\PY{n}{reward}\PY{p}{(}\PY{p}{)}
        \PY{n}{arms}\PY{p}{[}\PY{n}{Arm\PYZus{}index}\PY{p}{]}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{n}{reward}\PY{p}{)}

    \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Take
\((\alpha_1,\beta_1)=(1,1),(\alpha_2,\beta_2)=(1,1),(\alpha_3,\beta_3)=(1,1)\)
for example 1, where three arms respectively have true probabilities
0.8(green), 0.6(red) and 0.5(blue):

(Explanation of the legend: Each colored line corresponds two lines of
numbers. As for the first line, the format \(K/N\) means that the arm is
selected for \(N\) times in total and the number of reward id \(K\). As
for the second line, the format \((\alpha,\beta\)) means that the
present parameters with respect to Beta distribution of the arm are
\(\alpha\) and \(\beta\).)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Example 1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{TS}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project Report_files/Project Report_58_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{analysis-trade-off-problem-is-discussed-simultaneously}{%
\subsubsection{Analysis (Trade-off problem is discussed
simultaneously)}\label{analysis-trade-off-problem-is-discussed-simultaneously}}

    Note that for a continuous r.v. \(X\sim{\rm Beta}(\alpha,\beta)\), \[
E(X)=\frac{\alpha}{\alpha+\beta},{\rm Var}(X)=\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}\]
\[
\frac{\partial}{\partial\alpha}{\rm Var}(X)<0,\frac{\partial}{\partial\beta}{\rm Var}(X)<0
\] Since \(\alpha,\beta\) will be accumulated by the rewards as time
goes by, the variance will become smaller and smaller, and the PDF will
gradually become more and more slender unimodal functions, like the
green curve shown above.

Meanwhile, as a certain arm has been selected for many times, the peak
of its PDF will continuously converge to the real probability. By the
plot above, we know that as long as the arm with the largest reward
rate, denoted by \(I\), becomes the majority, it will continuously be
the majority afterwards. Therefore, for most of the trails, we will
select arm \(I\), which is extremely similar to the scenario of finding
the oracle value. That is why Thompson Sampling algorithm is highly
accurate.

That is to say, the whole process is another way to consider the
exploration-exploitation trade-off, totally different from the former
two algorithms. Essencially, Thompson sampling picks an arm randomly
according to its probability to be the best, so here is an efficient
strategy to simultaneously fully consider exploration and exploitation.

As for exploration, since the estimated reward is picked from a beta
distribution through randomization, it provides chances for the
potentially best arm to be chosen, like the process from Pic 3 to Pic 4
(in the figure above) in terms of the green curve. Besides, arms with
almost no chance to be the best are almost never chosen to avoid
spending time on useless exploration.

As for exploitation, it seems to be obvious. Consider the PDFs of three
arms, those with high probabilities to be the best are chosen most of
the time, so we can quickly lock in the best arm and keep selecting it
till the end, which can highly maximize the aggregate reward.

The real reason why Thompson sampling performs extremely well is that
since sampling from real distribution is always more accurate than
estimation, the selected arm is highly consistent with the optimal arm.
Therefore, the optimal arm can dominate all selections and the aggregate
reward can be very large.

    \hypertarget{conclusion}{%
\subsubsection{Conclusion}\label{conclusion}}

    We firstly discuss the common influence of \(\alpha\) and \(\beta\):

If \(\alpha\) and \(\beta\) are relatively small, like Ex.1, three arms
have similar probabilities to be selected, so the optimal arm can be
observed to be outstanding due to its high reward rate. In this
scenario, the optimal arm can always be observed, and the aggregate
reward is stable but no too closed to the oracle value.

If \(\alpha\) and \(\beta\) are both large, by the formula of variance,
the variance will be very small so exploitation dominates all strategies
and exploration seems to be a small probability event. Besides, since
\(E=\frac{\alpha}{\alpha+\beta}\), the PDF curve is tightly restricted
near the value of expectation, so the scope for randomization is greatly
limited. Therefore, whether we can get a perfect aggregate reward highly
depends on whether we are lucky. That is to say, if the arm with maximum
expectation is indeed the optimal arm, then the algorithm performs
extremely well and even exceeds the oracle value if fortunate, see
example 2. If not, the non-optimal arm will be selected over and over
again so the aggregate reward is relatively small, see example 3. Even
if there is intersection between green and red, the arm corresponding to
green is selected with extreme small probability (only twice out of the
first 500 time slots).

If some pairs are large and some are small, the aggregate reward also
depends on luck. Although scenarios are too various to be detailedly
discussed, we can generalize some general rules (denote the optimal arm
by \(I\)): As long as \((\alpha_I,\beta_I)\) are large and \(E(I)\) is
also relatively large or close to its real reward rate, or
\((\alpha_I,\beta_I)\) are relatively small, it still has high
probability to be selected and hence the aggregate reward is close to
the oracle value. However, if \((\alpha_I,\beta_I)\) are large but
\(E(I)\) is relatively small, the algorithm is more intended to select
other arms and hence the aggregate reward is not perfect any more.

    Example 2:
\((\alpha_1,\beta_1)=(801,201),(\alpha_2,\beta_2)=(401,601),(\alpha_3,\beta_3)=(601,401)\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Alpha} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{801}\PY{p}{,} \PY{l+m+mi}{401}\PY{p}{,} \PY{l+m+mi}{601}\PY{p}{]}\PY{p}{)}
\PY{n}{Beta} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{201}\PY{p}{,} \PY{l+m+mi}{601}\PY{p}{,} \PY{l+m+mi}{401}\PY{p}{]}\PY{p}{)}
\PY{n}{arms} \PY{o}{=} \PY{p}{[}\PY{n}{ARM}\PY{p}{(}\PY{n}{prob}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{i}\PY{p}{,} \PY{n}{Alpha}\PY{p}{,} \PY{n}{Beta}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{]}
\PY{n}{plt}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Example 2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{TS}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project Report_files/Project Report_64_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Example 3:
\((\alpha_1,\beta_1)=(361,631),(\alpha_2,\beta_2)=(401,601),(\alpha_3,\beta_3)=(201,801)\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Alpha} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{361}\PY{p}{,} \PY{l+m+mi}{401}\PY{p}{,} \PY{l+m+mi}{201}\PY{p}{]}\PY{p}{)}
\PY{n}{Beta} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{631}\PY{p}{,} \PY{l+m+mi}{601}\PY{p}{,} \PY{l+m+mi}{801}\PY{p}{]}\PY{p}{)}
\PY{n}{arms} \PY{o}{=} \PY{p}{[}\PY{n}{ARM}\PY{p}{(}\PY{n}{prob}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{i}\PY{p}{,} \PY{n}{Alpha}\PY{p}{,} \PY{n}{Beta}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{]}
\PY{n}{plt}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Example 3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{TS}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project Report_files/Project Report_66_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Then we discuss \(\alpha\) and \(\beta\) separately.

For fixed \(\beta\), the larger \(\alpha\) is, the larger the
expectation is and the smaller the variance is, so the scope of
exploration is already small. As shown in example 4, if \(\alpha\) is
relatively too large, it is difficult to distinguish which arm to choose
initially so the probability of an arm being chosen is evenly
distributed and therefore the non-optimal arms will be selected more,
leading to less aggregate reward. However, if \(\alpha\) is not too
large, like \((\alpha_1,\alpha_2,\alpha_3)=(70,50,60)\), the variances
are not too small as well so we can easily find the optimal arm, see
example 5.

    Example 4:
\((\alpha_1,\beta_1)=(700,2),(\alpha_2,\beta_2)=(500,2),(\alpha_3,\beta_3)=(600,2)\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Alpha} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{700}\PY{p}{,} \PY{l+m+mi}{500}\PY{p}{,} \PY{l+m+mi}{600}\PY{p}{]}\PY{p}{)}
\PY{n}{Beta} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\PY{n}{arms} \PY{o}{=} \PY{p}{[}\PY{n}{ARM}\PY{p}{(}\PY{n}{prob}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{i}\PY{p}{,} \PY{n}{Alpha}\PY{p}{,} \PY{n}{Beta}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{]}
\PY{n}{plt}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Example 4}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{TS}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project Report_files/Project Report_69_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Example 5:
\((\alpha_1,\beta_1)=(70,2),(\alpha_2,\beta_2)=(50,2),(\alpha_3,\beta_3)=(60,2)\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Alpha} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{70}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{60}\PY{p}{]}\PY{p}{)}
\PY{n}{Beta} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\PY{n}{arms} \PY{o}{=} \PY{p}{[}\PY{n}{ARM}\PY{p}{(}\PY{n}{prob}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{i}\PY{p}{,} \PY{n}{Alpha}\PY{p}{,} \PY{n}{Beta}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{]}
\PY{n}{plt}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Example 5}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{TS}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project Report_files/Project Report_71_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    For fixed \(\alpha\), the larger \(\beta\) is, the smaller the
expectation and variance is. If \(\beta\) is not too large, like example
6, we can easily distinguish the optimal arm even if its expectation is
smaller and then repeatedly select it so that the aggregate reward is
perfect. However, if \(\beta\) is large, since the variances are very
small, whether the optimal arm can dominate all selections depends on
whether its expectation is not the minimum. If \(E(I)\) is the minimum,
other non-optimal arms will be selected more and the aggregate reward is
not perfect, see example 7.

    Example 6:
\((\alpha_1,\beta_1)=(2,70),(\alpha_2,\beta_2)=(2,60),(\alpha_3,\beta_3)=(2,60)\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Alpha} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\PY{n}{Beta} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{70}\PY{p}{,} \PY{l+m+mi}{60}\PY{p}{,} \PY{l+m+mi}{60}\PY{p}{]}\PY{p}{)}

\PY{n}{arms} \PY{o}{=} \PY{p}{[}\PY{n}{ARM}\PY{p}{(}\PY{n}{prob}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{i}\PY{p}{,} \PY{n}{Alpha}\PY{p}{,} \PY{n}{Beta}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{]}
\PY{n}{plt}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Example 6}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{TS}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project Report_files/Project Report_74_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Example 7:
\((\alpha_1,\beta_1)=(2,700),(\alpha_2,\beta_2)=(2,600),(\alpha_3,\beta_3)=(2,600)\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Alpha} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\PY{n}{Beta} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{700}\PY{p}{,} \PY{l+m+mi}{600}\PY{p}{,} \PY{l+m+mi}{600}\PY{p}{]}\PY{p}{)}

\PY{n}{arms} \PY{o}{=} \PY{p}{[}\PY{n}{ARM}\PY{p}{(}\PY{n}{prob}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{i}\PY{p}{,} \PY{n}{Alpha}\PY{p}{,} \PY{n}{Beta}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{]}
\PY{n}{plt}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Example 7}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{TS}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project Report_files/Project Report_76_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{multi-armed-bandit-with-dependent-arms}{%
\section{Multi-armed Bandit with Dependent
Arms}\label{multi-armed-bandit-with-dependent-arms}}

\hypertarget{problem-formulation}{%
\subsection{Problem Formulation}\label{problem-formulation}}

Suppose that a mysterious fixed real parameter \(\lambda\in(0,1)\) is
hidden in the bandit, controlling the mean rewards of arms. That is, if
we pull arm \(I(t)\in\{1,2,3\}\) at time slot \(t\) and get a reward
\(r_{I(t)}\), \(r_{I(t)}\) is a r.v. satisfying Bernoulli distribution
with mean \(\theta_{I(t)}(\lambda)\), i.e., \[
r_{I(t)}=\begin{cases}
1,&w.p.\ \theta_{I(t)}(\lambda)\\
0,&w.p.\ 1-\theta_{I(t)}(\lambda)
\end{cases}
\] where \(\theta_{I(t)}(\lambda)\) is a funciton of \(\lambda\) and
\(\theta_{I(t)}(\lambda)\) is fixed if \(\lambda\) is given.

Suppose that \(\lambda\) is unknown but the functions
\(\{\theta_1(\lambda),\theta_2(\lambda),\theta_3(\lambda)\}:\lambda\to\mathbb{R}\)
are known to the player, i.e., for each given parameter \(\lambda\),
\(\theta_k(\lambda)\) can be uniquely determined so we can draw its
function plot over domain \((0,1)\).

Since \(\lambda\) is unknown to us, we can use a continuous random
variable \(X\sim{\rm Beta}(a,b)\) to estimate \(\lambda\), where \(a,b\)
are known positive constants. Therefore, the outcomes of three arms are
no longer independent, but they are conditionally independent given
\(X=\lambda\).

\hypertarget{problem-analysis}{%
\subsection{Problem Analysis}\label{problem-analysis}}

There are two steps of vital importance:

\begin{itemize}
\item
  Determine a reasonable condifence interval of \(\lambda\) in which the
  true parameter \(\lambda^*\) lies with high confidence.
\item
  In the confidence interval of \(\lambda\), determine which arms can be
  potentially selected, i.e., we should ignore those arms with
  relatively low rate of reward, and only determine which to select in a
  great arm set.
\end{itemize}

Since \(\lambda\in(0,1)\) and rewards satisfy Bernoulli distributions
given \(X=\lambda\), it is natural to consider Beta-Binomial conjugacy
to update the posterior distribution of \(X\). However, by mathematical
derivation, since the expectation of reward is a function
\(\theta(\lambda)\) rather than \(\lambda\), the posterior distribution
is no longer a beta distribution, so we cannnot depend on statistical
conjugacy to approximate the true parameter.

\hypertarget{algorithm-design}{%
\subsection{Algorithm Design}\label{algorithm-design}}

The new algorithm with respect to dependent arms is called UCB-DE.

\hypertarget{determine-confidence-interval-of-lambda}{%
\subsubsection{\texorpdfstring{Determine Confidence Interval of
\(\lambda\)}{Determine Confidence Interval of \textbackslash lambda}}\label{determine-confidence-interval-of-lambda}}

Before finding confidence interval, we should determine which arm should
be used as standard. Apply the idea of exploitation, generally the arm
with the most frequent selections is relatively the optimal arm. Denote
its index by \(I^{opt}\),
\(I^{opt}=\mathop{\arg\max}\limits_{i\in\{1,2,3\}}n_i\), where \(n_i\)
is the number of selections of arm \(i\) until time slot \(t\). We also
get its estimated rate of reward \(\hat{\theta}({I^{opt}})\).

Based on \(I^{opt}\), by Hoeffding Bound, \(\forall\epsilon>0\), \[
P(|\theta_{I^{opt}}(\lambda)-\hat{\theta}({I^{opt}})|\geqslant\epsilon)=2e^{-2n_{I^{opt}}\epsilon^2}
\] Let \(\delta=2e^{-2n_{I^{opt}}\epsilon^2}\) we have
\(\epsilon=\sqrt{\frac{\ln(2/\delta)}{2n_{I^{opt}}}}\). Take
\(\delta=2t^{-4}\) we have
\(\epsilon=\sqrt{\frac{2\ln(t)}{n_{I^{opt}}}}\), the \(1-\delta\)
confidence interval of \(\theta_{I^{opt}}(\lambda)\) is
\[\left[\hat{\theta}({I^{opt}})-\sqrt{\frac{2\ln(t)}{n_{I^{opt}}}},\hat{\theta}({I^{opt}})+\sqrt{\frac{2\ln(t)}{n_{I^{opt}}}}\right]\]

Therefore, the confidence interval of \(\lambda\), denoted by
\(\Lambda\), is \[
\Lambda=\left\{\lambda:|\theta_{I^{opt}}(\lambda)-\hat{\theta}({I^{opt}})|\leqslant\sqrt{\frac{2\ln(t)}{n_{I^{opt}}}}\right\}
\]

\hypertarget{filter-competitive-arms}{%
\subsubsection{Filter Competitive Arms}\label{filter-competitive-arms}}

We define a competitive set of arms, where arm \(j\) in the set
satisfying \(\exists\lambda\in\Lambda\) s.t.
\(\theta_j(\lambda)=\max\limits_{i\in\{1,2,3\}}\theta_i(\lambda)\).
Accordingly, those that should be ignored, whose index is \(k\) for
example, satisfies
\(\forall\lambda\in\Lambda,\theta_j(\lambda)<\max\limits_{i\in\{1,2,3\}}\theta_i(\lambda)\).
That is, whatever \(\lambda\) is, selecting these arms must be
non-optimal because there already exists an optimal arm to replace them.
After filtering we obtain a competitive set, denoted by \(\mathcal{C}\).

\hypertarget{apply-ucb-algorithm-process}{%
\subsubsection{Apply UCB Algorithm
Process}\label{apply-ucb-algorithm-process}}

We only apply classical UCB algorithm process to \(\mathcal{C}\), the
set of ``candidates''. At each time slot \(t\), we select the arm,
denoted by \(I(t)\), in \(\mathcal{C}\) with maximum upper confidence
bound \(\hat{\theta}(j)+c\sqrt{\frac{2\ln(t)}{n_j}}\), where \(c\) is a
positive constant with a default value of 1. After that, pull the
selected arm and update both \(n_j\) and \(\hat{\theta}(j)\).

The pseudocode is shown below, explaining more detailed algorithm
procedure.

\begin{algorithm}
\caption{UCB-DE}
\begin{algorithmic}[1]
    \Require $\theta_1(\lambda),\theta_2(\lambda),\theta_3(\lambda)$
    \For {$t=1,2,3$}
    \State $I(t)\gets t$
    \State $n_{I(t)}\gets 1$
    \State $\hat{\theta}_{I(t)}\gets r_{I(t)}$
    \EndFor
    \For {$t=4\to N$}
    \State $I^{opt}=\mathop{\arg\max}\limits_{i\in\{1,2,3\}}n_i$
    \State $\Lambda=\left\{\lambda:|\theta_{I^{opt}}(\lambda)-\hat{\theta}({I^{opt}})|\leqslant\sqrt{\frac{2\ln(t)}{n_{I^{opt}}}}\right\}$
    \State $\mathcal{C}=\left\{j:\theta_j(\lambda)=\max\limits_{i\in\{1,2,3\}}\theta_i(\lambda)\text{ for some }\lambda\in(0,1)\right\}$
    \State $I(t)\gets\mathop{\arg\max}\limits_{i\in\mathcal{C}}\left(\hat{\theta}_i+c\sqrt{\frac{2\ln(t)}{n_i}}\right)$
    \State $n_{I(t)}\gets n_{I(t)}+1$
    \State $\hat{\theta}_{I(t)}\gets\hat{\theta}_{I(t)}+\frac{r_{I(t)}-\hat{\theta}_{I(t)}}{n_{I(t)}}$
    \EndFor
\end{algorithmic}
\end{algorithm}

    \hypertarget{multi-armed-bandit-with-constraints}{%
\section{Multi-armed Bandit with
Constraints}\label{multi-armed-bandit-with-constraints}}

\hypertarget{problem-formulation}{%
\subsection{Problem Formulation}\label{problem-formulation}}

Suppose that all arms are independent with each other, but pulling any
arm will generate a fixed cost and the budget we have is limited.

Denote \(c_i\) as the fixed cost of the \(i\)th arm, which are known to
us, and \(W\) as the total budget we have initially. At each time slot,
denoted by \(t\), we denote \(W_t\) as the residual budget at time
\(t\).

We want to maximize the aggregate reward without exceeding the limited
budget. That is, we need to maximize
\(\sum\limits_{i=1}^3n_i\hat{\theta}_i\) and satisfy
\(\sum\limits_{i=1}^3n_ic_i\leqslant W_t\) at the same time, where
\(n_i\) is the number of how many time that arm \(i\) is pulled until
time slot \(t\).

\hypertarget{problem-analysis}{%
\subsection{Problem Analysis}\label{problem-analysis}}

The problem scenario seems to be very similar to a classical model we
have learnt: Knapsack problem! That is because, consider 3 categories of
objects with unlimited items, where each item belonging to the \(j\)th
category weighs \(c_j\), whose value is a random variable satisfying
\({\rm Bern}(\theta_j)\). The volumn of the knapsack is \(W\), we want
to maximize the aggregate value without exceeding the volumn of
knapsack. This is an unbounded knapsack problem, which is very similar
to the original problem. The unbounded knapsack problem with random
reward can be solved by stochastic dynamic programming.

However, since we do not know the exact values of \(\theta_j\)'s, they
have to be estimated parallel to pulling arms. Therefore we cannot apply
stochastic dynamic programming to solve the original problem.

Fortunately, an optimal approximation to unbounded knapsack problem is
considering fraction, i.e., suppose that the estimated rate of reward of
arm \(j\) is \(\hat{\theta}_j\), we turn to consider
\(\frac{\hat{\theta}_j}{c_j}\) in UCB algorithm to determine which arm
to choose at each time slot. The fraction reflects the value density of
an arm, i.e.~how much value per unit cost contains. Considering the arm
with maximum value density is a excellent way to find optimal strategy.

\hypertarget{algorithm-design}{%
\subsection{Algorithm Design}\label{algorithm-design}}

We propose a new algorithm with respect to the problem with constriants,
named UCB-LB, to solve the problem with finite budget and fixed arm
costs.

The general framework of UCB-LB algorithm is very similar to UCB
algorithm, but at each time slot \(t\), we need to construct a set
\(\mathcal{S}=\{i:c_i\leqslant W_t\}\), where our dicision over time
slot \(t\) must come from \(\mathcal{S}\). Then, denote the selected arm
by \(I(t)\), \[
I(t)=\mathop{\arg\max}\limits_{i\in\mathcal{S}}\left(\frac{\hat{\theta}_i+C\sqrt{\frac{2\ln(t)}{n_i}}}{c_i}\right)
\] where \(C\) is confidence value, which is a hyperparameter.

Like UCB algorithm, \(\hat{\theta}_i+C\sqrt{\frac{2\ln(t)}{n_i}}\) is
the upper bound of confidence interval. Combined with the denominator
\(c_i\), the fraction shows the upper bound of confidence interval in
density sense.

After selecting the arm, in spite of updating \(\hat\theta_{I(t)}\) and
\(n_{I(t)}\), we need to update the residual budget at time slot
\(t+1\), which is \(W_{t+1}=W_t-c_{I(t)}\).

As for initialization, all three arms are pulled exactly once separately
as what UCB does, but not in random order. We prefer to pulling the arms
by ``smaller cost, higher priority'' strategy, i.e., the smaller the
cost of any arm is, we tend to pull it with higher priority. Consider
the limited budget, such greedy strategy can ensure that the initial
three steps can get relatively more rewards to some extent.

The pseudocode is shown below, explaining more detailed algorithm
procedure.
\pagebreak
\begin{algorithm}
\caption{UCB-LB}
\begin{algorithmic}[1]
    \Ensure Sort arms in ascending order of cost.
    \State $W_1=W$
    \For {$t=1,2,3$}
    \State $I(t)\gets t$
    \State $n_{I(t)}\gets 1$
    \State $\hat{\theta}_{I(t)}\gets r_{I(t)}$
    \State $W_{t+1}=W_t-c_{I(t)}$
    \EndFor
    \For {$t=4\to N$}
    \State $\mathcal{S}=\Big\{i:c_i\leqslant W_t,i\in\{1,2,3\}\Big\}$
    \If {$\mathcal{S}=\varnothing$}
    \State Terminate the algorithm
    \EndIf
    \State $I(t)\gets\mathop{\arg\max}\limits_{i\in\mathcal{S}}\left(\frac{\hat{\theta}_i+C\sqrt{\frac{2\ln(t)}{n_i}}}{c_i}\right)$
    \State $n_{I(t)}\gets n_{I(t)}+1$
    \State $\hat{\theta}_{I(t)}\gets\hat{\theta}_{I(t)}+\frac{r_{I(t)}-\hat{\theta}_{I(t)}}{n_{I(t)}}$
    \State $W_{t+1}=W_t-c_{I(t)}$
    \EndFor
\end{algorithmic}
\end{algorithm}
    

\end{document}
